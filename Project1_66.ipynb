{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/643020514-7/643020514-7/blob/main/Project1_66.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3f0f914",
      "metadata": {
        "id": "e3f0f914"
      },
      "source": [
        "# Text Analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f8afa7f",
      "metadata": {
        "id": "6f8afa7f"
      },
      "source": [
        "<img src=\"https://www.datanami.com/wp-content/uploads/2014/06/text-analytics.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7866bdbc",
      "metadata": {
        "id": "7866bdbc"
      },
      "source": [
        "## Due Date: Sunday, October 1, 2023\n",
        "<br>\n",
        "<span style=\"color:red\">NOTE: There are always last minute issues submitting the case studies. DO NOT WAIT UNTIL THE LAST MINUTE!</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "830d61bd",
      "metadata": {
        "id": "830d61bd"
      },
      "source": [
        "## List team members:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f280656e",
      "metadata": {
        "id": "f280656e"
      },
      "source": [
        "1. นายภูริศ เครืชารี 643020514-7 <phuris.k@kkumail.com>\n",
        "2. นายธนพร ก้านกื่ง 643021264-9 <Thanaphorn.ka@kkumail.com>\n",
        "3. นางสาวพิมพิกา ยอดศรี 643020511-3   <phimphika.y@kkumail.com>\n",
        "4. นายปิยพัทธ์ ปานะถึก 643020507-4 <Piyaphat.p@kkumail.com>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3306068f",
      "metadata": {
        "id": "3306068f"
      },
      "source": [
        "**NOTE1**: Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost\n",
        "\n",
        "**NOTE2**: Create a slide presentation once finished, convert to pdf format, and turn in by one group member only\n",
        "<br>A list of documents to turn in: 1) Jupyter notebook containing results and 2) A set of slides in pdf format"
      ]
    },
    {
      "cell_type": "raw",
      "id": "7d719e86",
      "metadata": {
        "id": "7d719e86"
      },
      "source": [
        "##!pip3 install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install wordcloud"
      ],
      "metadata": {
        "id": "-5M0-GgET2lT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6c010d-c276-476d-8ce8-4c2335ec7824"
      },
      "id": "-5M0-GgET2lT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.2)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install twitter-scraper-selenium"
      ],
      "metadata": {
        "id": "3uIfrChQbQ1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c289fc8-b081-43bb-a0d4-30df5025b6f1"
      },
      "id": "3uIfrChQbQ1i",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting twitter-scraper-selenium\n",
            "  Downloading twitter_scraper_selenium-5.0.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from twitter-scraper-selenium) (2.8.2)\n",
            "Collecting selenium==4.7.0 (from twitter-scraper-selenium)\n",
            "  Downloading selenium-4.7.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting selenium-wire==5.1.0 (from twitter-scraper-selenium)\n",
            "  Downloading selenium_wire-5.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting webdriver-manager==3.2.2 (from twitter-scraper-selenium)\n",
            "  Downloading webdriver_manager-3.2.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting fake-headers==1.0.2 (from twitter-scraper-selenium)\n",
            "  Downloading fake_headers-1.0.2-py3-none-any.whl (17 kB)\n",
            "Collecting requests==2.27.1 (from twitter-scraper-selenium)\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bs4 (from fake-headers==1.0.2->twitter-scraper-selenium)\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (from fake-headers==1.0.2->twitter-scraper-selenium) (1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil==2.8.2->twitter-scraper-selenium) (1.16.0)\n",
            "Collecting urllib3<1.27,>=1.21.1 (from requests==2.27.1->twitter-scraper-selenium)\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.27.1->twitter-scraper-selenium) (2023.7.22)\n",
            "Collecting charset-normalizer~=2.0.0 (from requests==2.27.1->twitter-scraper-selenium)\n",
            "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.27.1->twitter-scraper-selenium) (3.4)\n",
            "Collecting trio~=0.17 (from selenium==4.7.0->twitter-scraper-selenium)\n",
            "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium==4.7.0->twitter-scraper-selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: blinker>=1.4 in /usr/lib/python3/dist-packages (from selenium-wire==5.1.0->twitter-scraper-selenium) (1.4)\n",
            "Collecting brotli>=1.0.9 (from selenium-wire==5.1.0->twitter-scraper-selenium)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kaitaistruct>=0.7 (from selenium-wire==5.1.0->twitter-scraper-selenium)\n",
            "  Downloading kaitaistruct-0.10-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: pyasn1>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from selenium-wire==5.1.0->twitter-scraper-selenium) (0.5.0)\n",
            "Requirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.10/dist-packages (from selenium-wire==5.1.0->twitter-scraper-selenium) (23.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from selenium-wire==5.1.0->twitter-scraper-selenium) (3.1.1)\n",
            "Requirement already satisfied: pysocks>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from selenium-wire==5.1.0->twitter-scraper-selenium) (1.7.1)\n",
            "Collecting wsproto>=0.14 (from selenium-wire==5.1.0->twitter-scraper-selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Collecting zstandard>=0.14.1 (from selenium-wire==5.1.0->twitter-scraper-selenium)\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2>=4.0 (from selenium-wire==5.1.0->twitter-scraper-selenium)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe>=6.0 (from selenium-wire==5.1.0->twitter-scraper-selenium)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting configparser (from webdriver-manager==3.2.2->twitter-scraper-selenium)\n",
            "  Downloading configparser-6.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting crayons (from webdriver-manager==3.2.2->twitter-scraper-selenium)\n",
            "  Downloading crayons-0.4.0-py2.py3-none-any.whl (4.6 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2>=4.0->selenium-wire==5.1.0->twitter-scraper-selenium)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0 in /usr/local/lib/python3.10/dist-packages (from pyOpenSSL>=22.0.0->selenium-wire==5.1.0->twitter-scraper-selenium) (41.0.3)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium==4.7.0->twitter-scraper-selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium==4.7.0->twitter-scraper-selenium) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium==4.7.0->twitter-scraper-selenium)\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium==4.7.0->twitter-scraper-selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium==4.7.0->twitter-scraper-selenium) (1.1.3)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->selenium-wire==5.1.0->twitter-scraper-selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->fake-headers==1.0.2->twitter-scraper-selenium) (4.11.2)\n",
            "Collecting colorama (from crayons->webdriver-manager==3.2.2->twitter-scraper-selenium)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib->fake-headers==1.0.2->twitter-scraper-selenium) (0.5.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0->pyOpenSSL>=22.0.0->selenium-wire==5.1.0->twitter-scraper-selenium) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->fake-headers==1.0.2->twitter-scraper-selenium) (2.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0->pyOpenSSL>=22.0.0->selenium-wire==5.1.0->twitter-scraper-selenium) (2.21)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=d749e4d896d5cdc5aa368eb67dfbfebfbe79ebd9df3621615130f5dd343cd330\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: brotli, zstandard, urllib3, outcome, kaitaistruct, hyperframe, hpack, h11, configparser, colorama, charset-normalizer, wsproto, trio, requests, h2, crayons, bs4, webdriver-manager, trio-websocket, fake-headers, selenium, selenium-wire, twitter-scraper-selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.2.0\n",
            "    Uninstalling charset-normalizer-3.2.0:\n",
            "      Successfully uninstalled charset-normalizer-3.2.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.27.1 which is incompatible.\n",
            "yfinance 0.2.28 requires requests>=2.31, but you have requests 2.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed brotli-1.1.0 bs4-0.0.1 charset-normalizer-2.0.12 colorama-0.4.6 configparser-6.0.0 crayons-0.4.0 fake-headers-1.0.2 h11-0.14.0 h2-4.1.0 hpack-4.0.0 hyperframe-6.0.1 kaitaistruct-0.10 outcome-1.2.0 requests-2.27.1 selenium-4.7.0 selenium-wire-5.1.0 trio-0.22.2 trio-websocket-0.11.1 twitter-scraper-selenium-5.0.0 urllib3-1.26.16 webdriver-manager-3.2.2 wsproto-1.2.0 zstandard-0.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "configparser"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6b4adb1",
      "metadata": {
        "id": "f6b4adb1"
      },
      "source": [
        "# Problem 1: Working with Twitter Data and JSON file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b90b2963",
      "metadata": {
        "id": "b90b2963"
      },
      "source": [
        "We are working with a Twitter dataset in JSON format from `thailand_tweets.txt`.<br>\n",
        "The tweets were scraped using \"Thailand\" keyword on August 31, 2022.<br>\n",
        "We are going to examine the dataset and retrieve information from the JSON file.\n",
        "Most Twitter datasets provide only Tweet ID where we can retrieve tweets from tweet ID as follow.\n",
        "```\n",
        "twitter.com/anyuser/status/<tweet_id>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Load these libraries"
      ],
      "metadata": {
        "id": "gBlyy6XRkPln"
      },
      "id": "gBlyy6XRkPln"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pprint\n",
        "import pandas as pd\n",
        "from io import StringIO # using StringIO to prevent ValueError\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import urllib.request\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "mqo5zFOpkSPg"
      },
      "id": "mqo5zFOpkSPg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Mount with google drive"
      ],
      "metadata": {
        "id": "WezmnrQlkUww"
      },
      "id": "WezmnrQlkUww"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "w95J2YZBkdjI",
        "outputId": "b8a7f6c4-ff56-4e86-85b8-9e61146df545"
      },
      "id": "w95J2YZBkdjI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c5915cc",
      "metadata": {
        "id": "9c5915cc"
      },
      "source": [
        "#### Read JSON file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b52d1f",
      "metadata": {
        "scrolled": false,
        "id": "a6b52d1f"
      },
      "outputs": [],
      "source": [
        "# read the file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/drive/MyDrive/python warehouse/thailand_tweets.txt','r') # rename\n",
        "foo = file.read()\n",
        "d = json.loads(foo)"
      ],
      "metadata": {
        "id": "G5xsfdxVamx3"
      },
      "id": "G5xsfdxVamx3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the file\n",
        "d"
      ],
      "metadata": {
        "id": "f9OaHiZZ1NGE"
      },
      "id": "f9OaHiZZ1NGE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ec2115fb",
      "metadata": {
        "id": "ec2115fb"
      },
      "source": [
        "#### Pretty print json/dict object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cca51f21",
      "metadata": {
        "id": "cca51f21"
      },
      "outputs": [],
      "source": [
        "thai_tweets  = json.loads(d)\n",
        "pretty_json = json.dumps(thai_tweets, indent=25)\n",
        "print(pretty_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91864d4a",
      "metadata": {
        "id": "91864d4a"
      },
      "source": [
        "Possible approaches:\n",
        "* In fact, `pandas` has `pandas.read_json(<file_dir>)` function to read json file into dataframe\n",
        "* As we create a data dict, we can also read our data dict into a pandas dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b0e435",
      "metadata": {
        "id": "45b0e435"
      },
      "source": [
        "# Your report\n",
        "* The total number of tweets collected in the file:\n",
        "* The content of the first tweet:\n",
        "* Is the first tweet contained any hashtags or mentions?\n",
        "* Collect all hashtags related to Thailand from this data\n",
        "* Find the most popular tweets in your collection of tweets, i.e. the tweets with the largest number of retweet/replies/likes counts\n",
        "    * You are free to define your own popularity metric\n",
        "* Display the top 5 tweets that are the most popular among your collection\n",
        "* Create a word cloud of words in the contents; however, we note that this word cloud is not a good representation of Thailand as the data size is small"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. The total number of tweets collected in the file:"
      ],
      "metadata": {
        "id": "9b3dc416"
      },
      "id": "9b3dc416"
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(thai_tweets)) #how to 1"
      ],
      "metadata": {
        "id": "N3bqIuqibK0S"
      },
      "id": "N3bqIuqibK0S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_key = list(thai_tweets.keys())[:]  #how to 2\n",
        "\n",
        "num_all_key = len(all_key)\n",
        "print(f'จำนวนtweetsทั้งหมดคือ',num_all_key)"
      ],
      "metadata": {
        "id": "dM5RTaethf5J"
      },
      "id": "dM5RTaethf5J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. The content of the first tweet:"
      ],
      "metadata": {
        "id": "Mj4d4suD1LIn"
      },
      "id": "Mj4d4suD1LIn"
    },
    {
      "cell_type": "code",
      "source": [
        "first_key = list(thai_tweets.keys())[0]  #\n",
        "first_content = thai_tweets[first_key]['content']\n",
        "\n",
        "print(f'contentแรก คือ:',first_content)"
      ],
      "metadata": {
        "id": "JQpfGaOI1PXX"
      },
      "id": "JQpfGaOI1PXX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Is the first tweet contained any hashtags or mentions?"
      ],
      "metadata": {
        "id": "FNgwUwB81P4n"
      },
      "id": "FNgwUwB81P4n"
    },
    {
      "cell_type": "code",
      "source": [
        "first = thai_tweets[first_key]\n",
        "keys=['hashtags','mentions']                      ### ทดสอบ ###\n",
        "\n",
        "hm = list( map(first.get, keys) )\n",
        "\n",
        "\n",
        "if not any(isinstance(item, str) for item in hm):\n",
        "    print(\"empty\")\n",
        "else:\n",
        "    print(\"not empty\")\n",
        "\n"
      ],
      "metadata": {
        "id": "weacegsAmNAc"
      },
      "id": "weacegsAmNAc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# tweet แรกพร้อม hashtags และ mentions\n",
        "first_tweet_id = next(iter(thai_tweets))\n",
        "first_tweet_name = thai_tweets[first_tweet_id]['name']\n",
        "first_tweet_username = thai_tweets[first_tweet_id]['username']\n",
        "first_tweet_hashtags = thai_tweets[first_tweet_id]['hashtags']\n",
        "first_tweet_mentions = thai_tweets[first_tweet_id]['mentions']\n",
        "first_tweet_content = thai_tweets[first_tweet_id]['content']\n",
        "\n",
        "print(\"Username:\", (first_tweet_username))\n",
        "print(\"name:\", (first_tweet_name))\n",
        "print(\"ในเนื้อหา:\", (first_tweet_content))\n",
        "# ตรวจสอบว่าทวีตแรกมี hashtags และ mentionsไหม และ print ออกมา\n",
        "if first_tweet_hashtags:\n",
        "    print(\"Hashtags:\", ', '.join(first_tweet_hashtags))\n",
        "else:\n",
        "    print(\"ไม่มีแฮชแท็กในทวีตนี้.\")\n",
        "\n",
        "if first_tweet_mentions:\n",
        "    print(\"Mentions:\", ', '.join(first_tweet_mentions))\n",
        "else:\n",
        "    print(\"ไม่มีการกล่าวถึงในทวีตนี้.\")\n"
      ],
      "metadata": {
        "id": "PLz3iVwjE4km"
      },
      "id": "PLz3iVwjE4km",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Collect all hashtags related to Thailand from this data"
      ],
      "metadata": {
        "id": "YyeXlhg11Tsc"
      },
      "id": "YyeXlhg11Tsc"
    },
    {
      "cell_type": "code",
      "source": [
        "thailand_hashtags_data = []\n",
        "\n",
        "for tweet_id, tweet_data in thai_tweets.items():\n",
        "    content = tweet_data['content']\n",
        "    hashtags = [tag.strip('#') for tag in content.split() if tag.startswith('#')]  #หาตัวที่ขึ้นต้นด้วย #\n",
        "\n",
        "    # ตรวจสอบว่าเนื้อหากล่าวถึงประเทศไทยหรือไม่\n",
        "    if 'thailand' in content.lower():\n",
        "        thailand_hashtags = [tag for tag in hashtags]\n",
        "\n",
        "        thailand_hashtags_data.extend(thailand_hashtags)\n",
        "\n",
        "# ลบแฮชแท็กที่ซ้ำกันโดยแปลงรายการเป็นชุดแล้วกลับเป็นรายการ\n",
        "thailand_hashtags_data = list(set(thailand_hashtags_data))\n",
        "\n",
        "print(f'แฮชแท็กทั้งหมดที่เกี่ยวข้องกับประเทศไทยคือ:', thailand_hashtags_data)\n"
      ],
      "metadata": {
        "id": "8xlKuTbZ2Z0o"
      },
      "id": "8xlKuTbZ2Z0o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Find the most popular tweets in your collection of tweets, i.e. the tweets with the largest number of retweet/replies/likes counts\n",
        "* You are free to define your own popularity metric"
      ],
      "metadata": {
        "id": "KHKNAWr31VdB"
      },
      "id": "KHKNAWr31VdB"
    },
    {
      "cell_type": "code",
      "source": [
        "most_retweeted_tweet = None\n",
        "most_liked_tweet = None\n",
        "\n",
        "for tweet_id, data in thai_tweets.items():\n",
        "\n",
        "    replies = data.get('replies', 0)\n",
        "    retweets = data.get('retweets', 0)\n",
        "    likes = data.get('likes', 0)\n",
        "    username = data.get('username',0)\n",
        "\n",
        "\n",
        "    if most_retweeted_tweet is None or retweets > most_retweeted_tweet['retweets']:\n",
        "        most_retweeted_tweet = {'tweet_id': tweet_id, 'retweets': retweets,'username':username}\n",
        "\n",
        "\n",
        "\n",
        "    if most_liked_tweet is None or likes > most_liked_tweet['likes']:\n",
        "        most_liked_tweet = {'tweet_id': tweet_id, 'likes': likes,'username': username}\n",
        "\n",
        "\n",
        "print(\"Tweet ที่มี Retweet มากที่สุดคือของ:\",most_retweeted_tweet['username'],\"มียอด Retweet\",most_retweeted_tweet['retweets'])\n",
        "print(\"Tweet ที่มี Like มากที่สุดคือของ:\",most_liked_tweet['username'],\"มียอด Like\",most_liked_tweet['likes'])"
      ],
      "metadata": {
        "id": "anssIPop9aaq"
      },
      "id": "anssIPop9aaq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Display the top 5 tweets that are the most popular among your collection"
      ],
      "metadata": {
        "id": "SCfuGC6x1jgd"
      },
      "id": "SCfuGC6x1jgd"
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_tweets = sorted(thai_tweets.values(), key=lambda x: x['likes'], reverse=True)\n",
        "\n",
        "# top 5 tweets\n",
        "top_5_tweets = sorted_tweets[:5]\n",
        "for i, tweet in enumerate(top_5_tweets, start=1):\n",
        "    print(f\"Top {i} Tweet:\")\n",
        "    print(f\"Username: {tweet['username']}\")\n",
        "    print(f\"Likes: {tweet['likes']}\")\n",
        "    print(f\"Content: {tweet['content']}\")\n",
        "    print(f\"Tweet URL: {tweet['tweet_url']}\")\n",
        "    print(\"-\" * 95)\n"
      ],
      "metadata": {
        "id": "7yrjSi9Q1m12"
      },
      "id": "7yrjSi9Q1m12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Create a word cloud of words in the contents; however, we note that this word cloud is not a good representation of Thailand as the data size is small"
      ],
      "metadata": {
        "id": "8d9oXNVb1pcW"
      },
      "id": "8d9oXNVb1pcW"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "\n",
        "\n",
        "tweet_texts = [tweet['content'] for tweet in thai_tweets.values()]\n",
        "\n",
        "# รวมข้อความtweetทั้งหมดไว้ในสตริงเดียว\n",
        "combined_text = 'story'.join(tweet_texts)\n",
        "\n",
        "# ลบ URL ,@ และแฮชแท็ก\n",
        "combined_text = re.sub(r'http\\S+|@\\w+|#\\w+', '', combined_text)\n",
        "\n",
        "# สร้าง worldcloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis', max_words=100).generate(combined_text)\n",
        "\n",
        "# Plot WordCloud about story\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.title('Word Cloud of Story Related Tweets', fontsize=16)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0z_T78FVP3GT"
      },
      "id": "0z_T78FVP3GT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c2cf90e5",
      "metadata": {
        "id": "c2cf90e5"
      },
      "source": [
        "Note: ไม่ต้องรายงานส่วนนี้ในสไลด์"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccb8731c",
      "metadata": {
        "id": "ccb8731c"
      },
      "source": [
        "# Problem 2 Study Trip Advisor Hotel Reviews Sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9da6a800",
      "metadata": {
        "id": "9da6a800"
      },
      "source": [
        "We are working with the Trip Advisor Hotel Reviews dataset. You can see the source from https://www.kaggle.com/datasets/andrewmvd/trip-advisor-hotel-reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19996e72",
      "metadata": {
        "id": "19996e72"
      },
      "source": [
        "* Analyze the data to find out what make a hotel good or bad\n",
        "<br>You may create a tag of \"positive\", \"negative\", or \"neural\" sentiment first. After tagging each tweet with different sentiment, we can separate positive tweets and negative tweets. Then, it is easier to find our common patterns in positive tweets or good hotels, and vice versa."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#----------------------------------------------\n",
        "##Your code starts here\n",
        "- label each tweet as positive, negative, and neural sentiment\n",
        "- combine tweet texts in each type\n",
        "- tokenize texts\n",
        "- convert to lower case\n",
        "- remove stop words\n",
        "- remove any other stop words, like RT\n",
        "- remove punctuations\n",
        "- remove other symbols?\n",
        "- analyze word frequency in each sentiment type and so on\n",
        "- create some data visualization for the analysis and your presentation"
      ],
      "metadata": {
        "id": "G_qyS1UXPPGa"
      },
      "id": "G_qyS1UXPPGa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparator Test"
      ],
      "metadata": {
        "id": "_cyufy_HDlSR"
      },
      "id": "_cyufy_HDlSR"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "yWbkkNv-SH4M"
      },
      "id": "yWbkkNv-SH4M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hotel = pd.read_csv('/content/drive/MyDrive/python warehouse/tripadvisor_hotel_reviews.csv')"
      ],
      "metadata": {
        "id": "cTDyp18nSJzf"
      },
      "id": "cTDyp18nSJzf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hotel"
      ],
      "metadata": {
        "id": "BAkkOVyEFcwj"
      },
      "id": "BAkkOVyEFcwj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hotel.columns"
      ],
      "metadata": {
        "id": "vq3QWPjqSb9x"
      },
      "id": "vq3QWPjqSb9x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e78959",
      "metadata": {
        "id": "c1e78959"
      },
      "outputs": [],
      "source": [
        "# #สร้าง reviews,ratings\n",
        "# reviews = hotel['Review']\n",
        "# ratings = hotel['Rating']\n",
        "\n",
        "# # สร้าง DataFrame\n",
        "# df_test = pd.DataFrame({'Review': reviews, 'Rating': ratings})\n",
        "\n",
        "#ลบคำที่มีความยาวน้อยกว่า 2 :เป็นfunction ชื่อ remove_less_than_2\n",
        "def remove_less_than_2(words):\n",
        "    return [word for word in words if len(word) >= 2]\n",
        "\n",
        "# Tokenization, lowercase conversion, and removing stopwords, RT, จัดการเครื่องหมายต่างๆ\n",
        "def preprocess_text(text):\n",
        "    # Tokenization :ใช้ nltk.word_tokenize() เพื่อแยกข้อความใน text ออกเป็นคำๆ\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Convert to lowercase :แปลงทุกคำในลิสต์ words ให้อยู่ในรูปแบบตัวพิมพ์เล็กทั้งหมด\n",
        "    words = [word.lower() for word in words]\n",
        "\n",
        "    # Remove stopwords :เพื่อรวบรวมคำหยุด (stop words) ในภาษาอังกฤษ\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words] #ลบ stop_words ออกจาก words\n",
        "\n",
        "    #  จัดการเครื่องหมายใน:ตัดคำที่ไม่ใช่ตัวอักษร a-z หรือ A-Z ออก.\n",
        "    words = [re.sub(r'^RT$|[^a-zA-Z]', '', word) for word in words]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "# ใช้ str.split() เพื่อแยกข้อความในแต่ละรีวิวเป็นคำๆ ตามช่องว่าง และนำมาเก็บใน Series ใหม่ โค้ดนี้ใช้ .apply() เพื่อประมวลผลแต่ละรีวิวใน Series(Processed_Review) โดยใช้ฟังก์ชัน remove_less_than_2\n",
        "#โค้ดนี้ใช้ .str.join(' ') เพื่อรวมคำที่เหลือในแต่ละรีวิวเข้าด้วยกันโดยคั่นด้วยช่องว่าง เก็บผลลัพธ์ในคอลัมน์ 'Processed_Review'\n",
        "hotel['Processed_Review'] = hotel['Review'].str.split().apply(remove_less_than_2).str.join(' ')\n",
        "\n",
        "# Filter reviews ตาม rating\n",
        "positive_reviews = hotel[hotel['Rating'] > 3]\n",
        "neutral_reviews = hotel[hotel['Rating'] == 3]\n",
        "negative_reviews = hotel[hotel['Rating'] < 3]\n",
        "\n",
        "\n",
        "\n",
        "positive_test = []\n",
        "negative_test = []\n",
        "neutral_test = []\n",
        "\n",
        "# นำ text เข้าแต่ละ category\n",
        "for index, row in positive_reviews.iterrows():\n",
        "    positive_test.append(preprocess_text(row['Review']))\n",
        "\n",
        "for index, row in negative_reviews.iterrows():\n",
        "    negative_test.append(preprocess_text(row['Review']))\n",
        "\n",
        "for index, row in neutral_reviews.iterrows():\n",
        "    neutral_test.append(preprocess_text(row['Review']))\n",
        "\n",
        "# การหาความถี่ของคำ\n",
        "def word_frequency(text_list):\n",
        "    words_f = nltk.word_tokenize(\" \".join(text_list))\n",
        "    freq_dist = nltk.FreqDist(words_f)\n",
        "    return freq_dist\n",
        "\n",
        "positive_freq_test = word_frequency(positive_test)\n",
        "negative_freq_test = word_frequency(negative_test)\n",
        "neutral_freq_test = word_frequency(neutral_test)\n",
        "\n",
        "# Visualization: สร้าง Word Cloud\n",
        "def generate_word_cloud(freq_dist, title):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(freq_dist)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "generate_word_cloud(positive_freq_test, 'Positive Sentiment Word Cloud')\n",
        "generate_word_cloud(negative_freq_test, 'Negative Sentiment Word Cloud')\n",
        "generate_word_cloud(neutral_freq_test, 'Neutral Sentiment Word Cloud')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finals"
      ],
      "metadata": {
        "id": "BIVfZuzAoIQS"
      },
      "id": "BIVfZuzAoIQS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Libraries and Load Data"
      ],
      "metadata": {
        "id": "9DHuhfu7C7By"
      },
      "id": "9DHuhfu7C7By"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "4qFjeM0_sios"
      },
      "id": "4qFjeM0_sios",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hotel = pd.read_csv('/content/drive/MyDrive/python warehouse/tripadvisor_hotel_reviews.csv')"
      ],
      "metadata": {
        "id": "dkOeTxwTHtjN"
      },
      "id": "dkOeTxwTHtjN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##เริ่มต้นด้วยการรวมรีวิวและคะแนนจาก DataFrame ของโรงแรม:\n",
        "- โค้ดเริ่มต้นด้วยการสร้าง DataFrame ที่ชื่อ df ซึ่งประกอบด้วยข้อมูลรีวิว (reviews) และคะแนน (ratings) จาก DataFrame ของโรงแรม hotel."
      ],
      "metadata": {
        "id": "STRNZOu7C9_U"
      },
      "id": "STRNZOu7C9_U"
    },
    {
      "cell_type": "code",
      "source": [
        "#สร้าง reviews,ratings จาก hotel\n",
        "reviews = hotel['Review']\n",
        "ratings = hotel['Rating']\n",
        "\n",
        "# สร้าง DataFrame\n",
        "df = pd.DataFrame({'Review': reviews, 'Rating': ratings})\n",
        "\n"
      ],
      "metadata": {
        "id": "NoRZWKpgH2Qf"
      },
      "id": "NoRZWKpgH2Qf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##กำหนด Label ให้กับรีวิวตามคะแนน (Sentiment Labeling):\n",
        "- ใช้ฟังก์ชัน label_sentiment เพื่อกำหนด รีวิวว่าเป็น Positive, Negative, หรือ Neutral โดยอิงตามคะแนน (ratings) ที่รีวิวนั้นมี และเก็บผลลัพธ์ไว้ในคอลัมน์ Sentiment ใน DataFrame df."
      ],
      "metadata": {
        "id": "tlskAQq1DB5Z"
      },
      "id": "tlskAQq1DB5Z"
    },
    {
      "cell_type": "code",
      "source": [
        "# (กำกับการให้คะแนนโดยมี positive,negative,neutral)\n",
        "def label_sentiment(rating):\n",
        "    if rating >= 4:\n",
        "        return 'Positive'\n",
        "    elif rating <= 2:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "df['Sentiment'] = df['Rating'].apply(label_sentiment)"
      ],
      "metadata": {
        "id": "WrG8n9UkIDdl"
      },
      "id": "WrG8n9UkIDdl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##รวมรีวิวแต่ละกลุ่ม (Positive, Negative, Neutral):\n",
        "- นำรีวิวในแต่ละกลุ่ม (Positive, Negative, Neutral) ที่มี Label ตามกลุ่มนั้นๆ มารวมเป็นข้อความเดียวในตัวแปร positive_reviews_a, negative_reviews_a, และ neutral_reviews_a โดยใช้ , เป็นตัวคั่นระหว่างรีวิวแต่ละรีวิว."
      ],
      "metadata": {
        "id": "o78wMql-DG_o"
      },
      "id": "o78wMql-DG_o"
    },
    {
      "cell_type": "code",
      "source": [
        "# ทำการรวมข้อความของแค่ละreviews คือ positive,negative,neutral โดยใช้ , เป็นตัวคั่นระหว่างข้อความแต่ละรีวิว.\n",
        "positive_reviews_a = \" \".join(df[df['Sentiment'] == 'Positive']['Review'])\n",
        "negative_reviews_a = \" \".join(df[df['Sentiment'] == 'Negative']['Review'])\n",
        "neutral_reviews_a = \" \".join(df[df['Sentiment'] == 'Neutral']['Review'])\n",
        "\n",
        "# นำข้อความในแต่ละกลุ่มที่รวมไว้แล้วแยกออกมาเป็นรีวิวแต่ละรีวิวด้วยการใช้ , เป็นตัวคั่นและสร้าง DataFrame\n",
        "aa = positive_reviews_a.split(',')\n",
        "bb = negative_reviews_a.split(',')\n",
        "cc = neutral_reviews_a.split(',')"
      ],
      "metadata": {
        "id": "bjtBYlY2JC1z"
      },
      "id": "bjtBYlY2JC1z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##แยกรีวิวออกเป็นรีวิวแต่ละรีวิว:"
      ],
      "metadata": {
        "id": "qS0eqCpeDKFG"
      },
      "id": "qS0eqCpeDKFG"
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame แยกตามกลุ่ม\n",
        "df_pos = pd.DataFrame({'Review': aa})\n",
        "df_neg = pd.DataFrame({'Review': bb})\n",
        "df_neu = pd.DataFrame({'Review': cc})"
      ],
      "metadata": {
        "id": "Eirov4jyJM1F"
      },
      "id": "Eirov4jyJM1F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##นำข้อความรีวิวมาวิเคราะห์และใช้ SentimentIntensityAnalyzer:\n",
        "- ในขั้นตอนนี้, นำข้อความรีวิวที่รวมไว้แต่ละกลุ่มแยกออกมาเป็นแต่ละรีวิวโดยใช้ , เป็นตัวคั่น และสร้าง DataFrame แยกตามกลุ่มของรีวิว คือ df_pos, df_neg, และ df_neu.\n",
        "-ใช้ SentimentIntensityAnalyzer (sid) เพื่อทำการวิเคราะห์ความรู้สึกของรีวิวในแต่ละกลุ่ม โดยคำนวณคะแนนของความเป็น positive, negative, หรือ neutral และเก็บผลลัพธ์ไว้ใน compound_score1, compound_score2, และ compound_score3."
      ],
      "metadata": {
        "id": "In_Bs6SdDQCK"
      },
      "id": "In_Bs6SdDQCK"
    },
    {
      "cell_type": "code",
      "source": [
        "positive_reviews = []\n",
        "negative_reviews = []\n",
        "neutral_reviews = []\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Zip the three DataFrames together\n",
        "for review1, review2, review3 in zip(df_pos['Review'], df_neg['Review'],df_neu['Review']):\n",
        "    # Perform sentiment analysis on each review\n",
        "    sentiment_scores1 = sid.polarity_scores(review1)\n",
        "    sentiment_scores2 = sid.polarity_scores(review2)\n",
        "    sentiment_scores3 = sid.polarity_scores(review3)\n",
        "\n",
        "    compound_score1 = sentiment_scores1['compound']\n",
        "    compound_score2 = sentiment_scores2['compound']\n",
        "    compound_score3 = sentiment_scores3['compound']\n",
        "\n",
        "     # Combine positive reviews from all three DataFrames\n",
        "    if compound_score1 >= 0.1:\n",
        "        positive_reviews.append(review1)\n",
        "    if compound_score2 >= 0.1:\n",
        "        positive_reviews.append(review2)\n",
        "    if compound_score3 >= 0.1:\n",
        "        positive_reviews.append(review3)\n",
        "\n",
        "    # Combine negative reviews from all three DataFrames\n",
        "    if compound_score1 <= -0.1:\n",
        "        negative_reviews.append(review1)\n",
        "    if compound_score2 <= -0.1:\n",
        "        negative_reviews.append(review2)\n",
        "    if compound_score3 <= -0.1:\n",
        "        negative_reviews.append(review3)\n",
        "\n",
        "    # Combine neutral reviews from all three DataFrames\n",
        "    if -0.1 < compound_score1 < 0.1:\n",
        "        neutral_reviews.append(review1)\n",
        "    if -0.1 < compound_score2 < 0.1:\n",
        "        neutral_reviews.append(review2)\n",
        "    if -0.1 < compound_score3 < 0.1:\n",
        "        neutral_reviews.append(review3)"
      ],
      "metadata": {
        "id": "5z1QyuHdJps7"
      },
      "id": "5z1QyuHdJps7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##การทำความสะอาดและLemmatizeข้อความ (Text Preprocessing and Lemmatization):\n",
        "- ใช้ WordNet Lemmatizer เพื่อเปลี่ยนคำในรูปแบบพื้นฐาน (lemma) และนำไปลบ stopwords และอักขระที่ไม่ใช่ตัวอักษรที่ไม่เป็นตัวอักษร โดยผลลัพธ์จะเป็นรีวิวที่ถูกนำมาทำความสะอาดและเลมมาไทซ์แล้ว."
      ],
      "metadata": {
        "id": "Q8k1QGLBJtb9"
      },
      "id": "Q8k1QGLBJtb9"
    },
    {
      "cell_type": "code",
      "source": [
        "#การทำความสะอาดและlemmatizeข้อความ\n",
        "# Create a WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#  lemmatize text\n",
        "def preprocess_and_lemmatize(text):\n",
        "    # Tokenization\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Lemmatization\n",
        "    words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # ลบอักขระที่ไม่ใช่ตัวอักษร\n",
        "    words = [re.sub(r'[^a-zA-Z]', '', word) for word in words if word.isalpha()]\n",
        "\n",
        "    # Remove specific words like \"room\" and \"hotel\"\n",
        "    cleaned_tokens = [token for token in cleaned_tokens if token not in ['room', 'hotel']]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Apply preprocessing and lemmatization to reviews\n",
        "positive_reviews = [preprocess_and_lemmatize(review) for review in positive_reviews]\n",
        "negative_reviews = [preprocess_and_lemmatize(review) for review in negative_reviews]\n",
        "neutral_reviews = [preprocess_and_lemmatize(review) for review in neutral_reviews]\n"
      ],
      "metadata": {
        "id": "Y_wJ5GHdKAcY"
      },
      "id": "Y_wJ5GHdKAcY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##การหาความถี่ของคำ (Word Frequency Analysis):\n",
        "- ใช้ฟังก์ชัน word_frequency เพื่อหาความถี่ของคำในข้อความที่ผ่านการ preprocess แล้วในแต่ละกลุ่ม (positive, negative, neutral) โดยผลลัพธ์จะถูกนำมาสร้างเป็นกราฟความถี่ (word frequency) สำหรับแต่ละกลุ่มของรีวิว (positive, negative, neutral)."
      ],
      "metadata": {
        "id": "hOQImZXFKJMC"
      },
      "id": "hOQImZXFKJMC"
    },
    {
      "cell_type": "code",
      "source": [
        "# การหาความถี่ของคำ\n",
        "def word_frequency(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    freq_dist = nltk.FreqDist(words)\n",
        "    return freq_dist\n",
        "\n",
        "positive_freq_dist = word_frequency(\" \".join(positive_reviews))\n",
        "negative_freq_dist = word_frequency(\" \".join(negative_reviews))\n",
        "neutral_freq_dist = word_frequency(\" \".join(neutral_reviews))\n"
      ],
      "metadata": {
        "id": "Zq-XJReSKSpr"
      },
      "id": "Zq-XJReSKSpr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## การสร้าง Word Cloud:\n",
        "- ใช้ไลบรารี WordCloud เพื่อสร้าง Word Cloud สำหรับแสดงคำที่มีความถี่กลุ่มคำของรีวิว (positive, negative, neutral). Word Cloud"
      ],
      "metadata": {
        "id": "1UYiv3IhKKrl"
      },
      "id": "1UYiv3IhKKrl"
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# generate a word cloud\n",
        "def generate_word_cloud(text, title):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qzXK6s95KWLM"
      },
      "id": "qzXK6s95KWLM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##แสดงผล Word Cloud:\n",
        "- สุดท้าย, นำ Word Cloud ที่สร้างขึ้นมาแสดงผลสำหรับแต่ละกลุ่ม (positive, negative, neutral) เพื่อแสดงคำที่มีความถี่ของแต่ละกลุ่ม"
      ],
      "metadata": {
        "id": "-ME7aBO6KK_4"
      },
      "id": "-ME7aBO6KK_4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate word clouds for positive, negative, and neutral reviews\n",
        "generate_word_cloud(\" \".join(positive_reviews), 'Positive Sentiment Word Cloud')\n",
        "generate_word_cloud(\" \".join(negative_reviews), 'Negative Sentiment Word Cloud')\n",
        "generate_word_cloud(\" \".join(neutral_reviews), 'Neutral Sentiment Word Cloud')"
      ],
      "metadata": {
        "id": "KCjioDBsKbGj"
      },
      "id": "KCjioDBsKbGj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#report"
      ],
      "metadata": {
        "id": "-1hPd-e4k9ID"
      },
      "id": "-1hPd-e4k9ID"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count the number of reviews in each sentiment category\n",
        "positive_count = len(positive_reviews)\n",
        "negative_count = len(negative_reviews)\n",
        "neutral_count = len(neutral_reviews)\n",
        "\n",
        "# Create a list of counts\n",
        "sentiment_counts = [positive_count, negative_count, neutral_count]\n",
        "\n",
        "# Labels for each category\n",
        "labels = ['Positive', 'Negative', 'Neutral']\n",
        "\n",
        "# Colors for the pie chart\n",
        "colors = ['#66b3ff','#ff9999' , '#99ff99']\n",
        "\n",
        "# Create a pie chart\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(sentiment_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Sentiment Distribution of Reviews')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xckjDYnDpByF"
      },
      "id": "xckjDYnDpByF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate word frequencies for Positive reviews\n",
        "positive_freq_dist = word_frequency(\" \".join(positive_reviews))\n",
        "top_10_positive = positive_freq_dist.most_common(10)\n",
        "words_positive, frequencies_positive = zip(*top_10_positive)\n",
        "\n",
        "# Calculate word frequencies for Negative reviews\n",
        "negative_freq_dist = word_frequency(\" \".join(negative_reviews))\n",
        "top_10_negative = negative_freq_dist.most_common(10)\n",
        "words_negative, frequencies_negative = zip(*top_10_negative)\n",
        "\n",
        "# Calculate word frequencies for Neutral reviews\n",
        "neutral_freq_dist = word_frequency(\" \".join(neutral_reviews))\n",
        "top_10_neutral = neutral_freq_dist.most_common(10)\n",
        "words_neutral, frequencies_neutral = zip(*top_10_neutral)\n",
        "\n",
        "# Create a bar chart for Positive reviews\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.subplot(131)\n",
        "plt.barh(words_positive, frequencies_positive, color='#66b3ff')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Words')\n",
        "plt.title('Top 10 Most Frequent Words (Positive)')\n",
        "\n",
        "# Create a bar chart for Negative reviews\n",
        "plt.subplot(132)\n",
        "plt.barh(words_negative, frequencies_negative, color='lightcoral')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Words')\n",
        "plt.title('Top 10 Most Frequent Words (Negative)')\n",
        "\n",
        "# Create a bar chart for Neutral reviews\n",
        "plt.subplot(133)\n",
        "plt.barh(words_neutral, frequencies_neutral, color='#99ff99')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Words')\n",
        "plt.title('Top 10 Most Frequent Words (Neutral)')\n",
        "\n",
        "plt.tight_layout()  # Ensure proper spacing\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VgXYUgdIy0_V"
      },
      "id": "VgXYUgdIy0_V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate and display a bar chart for word frequencies\n",
        "def generate_combined_bar_chart(positive_freq_dist, negative_freq_dist, neutral_freq_dist):\n",
        "    top_words_positive, frequencies_positive = zip(*positive_freq_dist.most_common(10))  # Top 10 words in Positive Reviews\n",
        "    top_words_negative, frequencies_negative = zip(*negative_freq_dist.most_common(10))  # Top 10 words in Negative Reviews\n",
        "    top_words_neutral, frequencies_neutral = zip(*neutral_freq_dist.most_common(10))  # Top 10 words in Neutral Reviews\n",
        "\n",
        "    # Create a combined bar chart\n",
        "    bar_width = 0.2\n",
        "    index = range(len(top_words_positive))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(index, frequencies_positive, bar_width, label='Positive', alpha=0.8, color='#66b3ff')\n",
        "    plt.bar([i + bar_width for i in index], frequencies_negative, bar_width, label='Negative', alpha=0.8, color='lightcoral')\n",
        "    plt.bar([i + 2 * bar_width for i in index], frequencies_neutral, bar_width, label='Neutral', alpha=0.8, color='#99ff99')\n",
        "\n",
        "\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('Frequencies')\n",
        "    plt.title('Top 10 Words in Different Sentiment Categories')\n",
        "    plt.xticks([i + bar_width for i in index], top_words_positive, rotation=45)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate and display a combined bar chart for word frequencies in different sentiment categories\n",
        "generate_combined_bar_chart(positive_freq_dist, negative_freq_dist, neutral_freq_dist)\n"
      ],
      "metadata": {
        "id": "5gO1wSdGwrn5"
      },
      "id": "5gO1wSdGwrn5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the categories and colors\n",
        "categories = ['Positive (No compound analysis)','Positive (compound analysis)', 'Negative (No compound analysis)','Negative (compound analysis)', 'Neutral (No compound analysis)',   'Neutral (compound analysis)']\n",
        "colors = ['#80afd6', '#3b7fb9', '#d68080', '#9a3131', '#a5e2ae', '#5bca6b']\n",
        "\n",
        "# Define the data for each category\n",
        "data = [\n",
        "    positive_freq_test.most_common(10),\n",
        "    positive_freq_dist.most_common(10),\n",
        "    negative_freq_test.most_common(10),\n",
        "    negative_freq_dist.most_common(10),\n",
        "    neutral_freq_test.most_common(10),\n",
        "    neutral_freq_dist.most_common(10)\n",
        "]\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(3, 2, figsize=(14, 12))\n",
        "fig.suptitle('Top 10 Words in Sentiment Analysis', fontsize=16)\n",
        "\n",
        "for i in range(len(categories)):\n",
        "    ax = axs[i // 2, i % 2]\n",
        "    words, counts = zip(*data[i])\n",
        "    ax.barh(words, counts, color=colors[i])\n",
        "    ax.set_xlabel('Frequency')\n",
        "    ax.set_title(categories[i])\n",
        "    ax.invert_yaxis()  # Invert the y-axis for better readability\n",
        "\n",
        "# Set y-axis labels\n",
        "for ax in axs.flat:\n",
        "    ax.set(ylabel='Words')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0RXF7lYo4d45"
      },
      "id": "0RXF7lYo4d45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0b4cf7eb",
      "metadata": {
        "id": "0b4cf7eb"
      },
      "source": [
        "# Report\n",
        "* How did you analyze the data?\n",
        "* What did you find in the data? (please include figures or tables in the report, but no source code)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38795f4f",
      "metadata": {
        "id": "38795f4f"
      },
      "source": [
        "# Problem 3 Collect and Analyze Your Interesting Topic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d03d587",
      "metadata": {
        "id": "9d03d587"
      },
      "source": [
        "* Select a topic that your group members are interested\n",
        "* Gather url from at least 3 webpages\n",
        "* Use urllib.request to retrieve data from webpage\n",
        "* Clean and find intersting patterns and information\n",
        "* Create a word cloud of your topic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Topic\n",
        "##Wikipedia Text Analysis of Artificial Intelligence, Machine Learning, and Deep Learning"
      ],
      "metadata": {
        "id": "QqqvXBZJjlfe"
      },
      "id": "QqqvXBZJjlfe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Libraries and Load Data"
      ],
      "metadata": {
        "id": "M0FOgWfpXVFF"
      },
      "id": "M0FOgWfpXVFF"
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "pfbNFb50W9tp"
      },
      "id": "pfbNFb50W9tp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##กำหนด URL และชื่อเรื่องสำหรับบทความ Wikipedia"
      ],
      "metadata": {
        "id": "aD2MYpp9XbPZ"
      },
      "id": "aD2MYpp9XbPZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define URLs and Titles for Wikipedia Articles\n",
        "articles = [\n",
        "    {\"urls\": [\n",
        "        \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
        "        \"https://en.wikipedia.org/wiki/AI_in_video_games\",\n",
        "        \"https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare\"\n",
        "    ], \"title\": \"Artificial Intelligence\"},\n",
        "    {\"urls\": [\n",
        "        \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
        "        \"https://en.wikipedia.org/wiki/Supervised_learning\",\n",
        "        \"https://en.wikipedia.org/wiki/Unsupervised_learning\"\n",
        "    ], \"title\": \"Machine Learning\"},\n",
        "    {\"urls\": [\n",
        "        \"https://en.wikipedia.org/wiki/Deep_learning\",\n",
        "        \"https://en.wikipedia.org/wiki/Convolutional_neural_network\",\n",
        "        \"https://en.wikipedia.org/wiki/Recurrent_neural_network\"\n",
        "    ], \"title\": \"Deep Learning\"},\n",
        "]"
      ],
      "metadata": {
        "id": "tVY4yYsOf8Lo"
      },
      "id": "tVY4yYsOf8Lo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##การประมวลผลบทความ\n",
        "- การทำTokenization และการ clean\n",
        "- คำนวณความถี่ของคำ"
      ],
      "metadata": {
        "id": "BkwEJaVqgSTH"
      },
      "id": "BkwEJaVqgSTH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process an article and calculate word frequencies\n",
        "def process_article(urls):\n",
        "    text_data = \"\"\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = urllib.request.urlopen(url)\n",
        "\n",
        "            if response.status == 200:\n",
        "                page_content = response.read()\n",
        "                soup = BeautifulSoup(page_content, 'html.parser')\n",
        "\n",
        "                # Extract paragraphs from the article\n",
        "                paragraphs = soup.find_all('p')\n",
        "\n",
        "                # Concatenate text from all paragraphs\n",
        "                for paragraph in paragraphs:\n",
        "                    text_data += paragraph.text\n",
        "        except urllib.error.HTTPError as e:\n",
        "            print(f\"HTTPError occurred for URL {url}: {e}\")\n",
        "\n",
        "    # Tokenize and Clean Data\n",
        "    words = word_tokenize(text_data)\n",
        "    # Remove words with length less than 2 characters\n",
        "    words = [word.lower() for word in words if word.isalpha() and len(word) >= 2]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Calculate Word Frequencies\n",
        "    word_freq_dist = nltk.FreqDist(words)\n",
        "    return word_freq_dist\n",
        "\n",
        "# Process each article and calculate word frequencies\n",
        "all_freq_dist = process_article(articles[0][\"urls\"] + articles[1][\"urls\"] + articles[2][\"urls\"])\n",
        "ai_freq_dist = process_article(articles[0][\"urls\"])\n",
        "ml_freq_dist = process_article(articles[1][\"urls\"])\n",
        "dl_freq_dist = process_article(articles[2][\"urls\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "Go8tRS4fgTwg"
      },
      "id": "Go8tRS4fgTwg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create DataFrames to display word frequencies\n",
        "all_df = pd.DataFrame(list(all_freq_dist.items()), columns=[\"Word\", \"Frequency\"])\n",
        "ai_df = pd.DataFrame(list(ai_freq_dist.items()), columns=[\"Word\", \"Frequency\"])\n",
        "ml_df = pd.DataFrame(list(ml_freq_dist.items()), columns=[\"Word\", \"Frequency\"])\n",
        "dl_df = pd.DataFrame(list(dl_freq_dist.items()), columns=[\"Word\", \"Frequency\"])\n",
        "\n",
        "# Display the DataFrames\n",
        "print(\"All Articles:\")\n",
        "print(all_df.head())\n",
        "print(\"\\nAI Articles:\")\n",
        "print(ai_df.head())\n",
        "print(\"\\nML Articles:\")\n",
        "print(ml_df.head())\n",
        "print(\"\\nDL Articles:\")\n",
        "print(dl_df.head())\n"
      ],
      "metadata": {
        "id": "oYvuMyZFtZ45"
      },
      "id": "oYvuMyZFtZ45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_word_freq(freq_dist, title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    freq_dist.plot(20, title=title, cumulative=False)\n",
        "    plt.xlabel('Your Custom X-Axis Label')\n",
        "    plt.ylabel('Your Custom Y-Axis Label')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Plot word frequency graphs for all articles\n",
        "plot_word_freq(all_freq_dist, 'Word Frequency in All Articles')\n"
      ],
      "metadata": {
        "id": "zRyKFbdgYWMl"
      },
      "id": "zRyKFbdgYWMl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process an article and calculate word frequencies\n",
        "def process_article(urls):\n",
        "    text_data = \"\"\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = urllib.request.urlopen(url)\n",
        "\n",
        "            if response.status == 200:\n",
        "                page_content = response.read()\n",
        "                soup = BeautifulSoup(page_content, 'html.parser')\n",
        "\n",
        "                # Extract paragraphs from the article\n",
        "                paragraphs = soup.find_all('p')\n",
        "\n",
        "                # Concatenate text from all paragraphs\n",
        "                for paragraph in paragraphs:\n",
        "                    text_data += paragraph.text\n",
        "        except urllib.error.HTTPError as e:\n",
        "            print(f\"HTTPError occurred for URL {url}: {e}\")"
      ],
      "metadata": {
        "id": "1noK2AlGTQou"
      },
      "id": "1noK2AlGTQou",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##สร้าง Word Cloud"
      ],
      "metadata": {
        "id": "4yiO9wBtXqvt"
      },
      "id": "4yiO9wBtXqvt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Word Cloud\n",
        "def generate_word_cloud(freq_dist, title):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white',max_words=300).generate_from_frequencies(freq_dist)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(title,fontsize=25)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ESX6zK8lhIqF"
      },
      "id": "ESX6zK8lhIqF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##แสดง Word Cloud"
      ],
      "metadata": {
        "id": "qVACddIJXsfF"
      },
      "id": "qVACddIJXsfF"
    },
    {
      "cell_type": "code",
      "source": [
        "generate_word_cloud(all_freq_dist, 'All Articles Word Cloud')\n",
        "generate_word_cloud(ai_freq_dist, 'Artificial Intelligence Word Cloud')\n",
        "generate_word_cloud(ml_freq_dist, 'Machine Learning Word Cloud')\n",
        "generate_word_cloud(dl_freq_dist, 'Deep Learning Word Cloud')"
      ],
      "metadata": {
        "id": "ZXkvfSz5hKNz"
      },
      "id": "ZXkvfSz5hKNz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##สร้างแผนภูมิ Top 10 Word Frequency Charts"
      ],
      "metadata": {
        "id": "DboesSIghQ94"
      },
      "id": "DboesSIghQ94"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top 10 words and their frequencies for each category\n",
        "ai_top_words, ai_top_freqs = zip(*ai_freq_dist.most_common(10))\n",
        "ml_top_words, ml_top_freqs = zip(*ml_freq_dist.most_common(10))\n",
        "dl_top_words, dl_top_freqs = zip(*dl_freq_dist.most_common(10))\n",
        "\n",
        "# Define colors for each category\n",
        "ai_color = 'skyblue'\n",
        "ml_color = 'lightcoral'\n",
        "dl_color = 'lightgreen'\n",
        "\n",
        "# Create separate bar charts for each category with different colors\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Artificial Intelligence\n",
        "plt.subplot(131)\n",
        "plt.bar(ai_top_words, ai_top_freqs, color=ai_color)\n",
        "plt.title('Artificial Intelligence - Top 10 Word Frequencies')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Machine Learning\n",
        "plt.subplot(132)\n",
        "plt.bar(ml_top_words, ml_top_freqs, color=ml_color)\n",
        "plt.title('Machine Learning - Top 10 Word Frequencies')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Deep Learning\n",
        "plt.subplot(133)\n",
        "plt.bar(dl_top_words, dl_top_freqs, color=dl_color)\n",
        "plt.title('Deep Learning - Top 10 Word Frequencies')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mlCGp7dPhNlD"
      },
      "id": "mlCGp7dPhNlD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "40357580",
      "metadata": {
        "id": "40357580"
      },
      "source": [
        "### Report\n",
        "* What did you find out about your topic? (please include figures or tables in the report, but no source code)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a13de1d",
      "metadata": {
        "id": "8a13de1d"
      },
      "source": [
        "# ☃️ The End of Project 1 ☃️"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}